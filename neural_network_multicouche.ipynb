{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialisation(n0,n1,n2, nCouche):\n",
    "\n",
    "    WMultiple = []\n",
    "    bMultiple = []\n",
    "    WMultiple.append(np.random.randn(n1, n0))\n",
    "    bMultiple.append(np.zeros((n1, 1)))\n",
    "    for i in range(0, nCouche-2):\n",
    "        WMultiple.append(np.random.randn(n1, n1))\n",
    "        bMultiple.append(np.zeros((n1, 1)))\n",
    "    WMultiple.append(np.random.randn(n2, n1))\n",
    "    bMultiple.append(np.zeros((n2, 1)))\n",
    "\n",
    "    return WMultiple, bMultiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parametres):\n",
    "\n",
    "    #on récupère les paramètres pour nos 2 couches\n",
    "    WMultiple, bMultiple = parametres\n",
    "    AMultiple = []\n",
    "    for i in range(len(WMultiple)):\n",
    "            #on calcule les activations de la première couche \n",
    "        if i == 0:\n",
    "            ZMultiple = WMultiple[i].dot(X) + bMultiple[i]\n",
    "            AMultiple.append(1 / (1 + np.exp(-ZMultiple)))\n",
    "        else:\n",
    "             #on calcule les activations de la deuxième couche à partir de l'activation de la première couche\n",
    "            ZMultiple = WMultiple[i].dot(AMultiple[-1]) + bMultiple[i]\n",
    "            AMultiple.append(1 / (1 + np.exp(-ZMultiple)))\n",
    "        \n",
    "    return AMultiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(X, y, parametres, activations):\n",
    "\n",
    "    WMultiple = parametres[0]\n",
    "\n",
    "    # A1 = activations['A1']\n",
    "    # A2 = activations['A2']\n",
    "    Wlast = WMultiple[-1]\n",
    "\n",
    "    m = y.shape[1]\n",
    "\n",
    "    dZMultiple = []\n",
    "    dbMultiple = []\n",
    "    dWMultiple = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(WMultiple)-1, -1, -1):\n",
    "        print(\"i\",i)\n",
    "        aIndex = i-1\n",
    "        if i == (len(WMultiple)-1):\n",
    "            dZMultiple.append(activations[i] - y)\n",
    "            dWMultiple.append(1 / m * dZMultiple[-1].dot(activations[-2].T))\n",
    "            dbMultiple.append(1 / m * np.sum(dZMultiple[-1], axis=1, keepdims = True))\n",
    "        elif i == 0:\n",
    "            print(\"W shape\",WMultiple[1].shape)\n",
    "            print(\"dZ shape\",dZMultiple[-1].shape)\n",
    "            #pour la premiere couche on a besoin des poids de la deuxieme couche, du coup l'index 1 de WMultiple\n",
    "            dZMultiple.append(np.dot(WMultiple[1].T, dZMultiple[-1]) * activations[i] * (1 - activations[i]))\n",
    "            dWMultiple.append(1 / m * dZMultiple[-1].dot(X.T))\n",
    "            dbMultiple.append(1 / m * np.sum(dZMultiple[-1], axis=1, keepdims = True))\n",
    "        else:\n",
    "            dZMultiple.append(np.dot(WMultiple[-i].T, dZMultiple[-1]) * activations[aIndex] * (1 - activations[aIndex]))\n",
    "            dWMultiple.append(1 / m * dZMultiple[-1].dot(activations[aIndex].T))\n",
    "            dbMultiple.append(1 / m * np.sum(dZMultiple[-1], axis=1, keepdims = True))\n",
    "\n",
    "\n",
    "    # Avant de retourner les gradients, on inverse l'ordre des listes\n",
    "    dWMultiple = dWMultiple[::-1]\n",
    "    dbMultiple = dbMultiple[::-1]\n",
    "    #affichage de chaque gradient\n",
    "    for i in range(len(WMultiple)):\n",
    "        print(\"WMultiple\",WMultiple[i].shape)\n",
    "        \n",
    "    for i in range(len(dWMultiple)):\n",
    "        print(\"dWMultiple\",dWMultiple[i].shape) \n",
    "    \n",
    "    for i in range(len(dbMultiple)):\n",
    "        print(\"dbMultiple\",dbMultiple[i].shape)\n",
    "    # dZ3 = A3 - y\n",
    "    # dW3 = 1 / m * dZ3.dot(A2.T)\n",
    "    # db3 = 1 / m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    # dZ2 = np.dot(Wt.T, dZ3) * A2 * (1 - A2)\n",
    "\n",
    "    # dZ1 = np.dot(W2.T, dZ2) * A1 * (1 - A1)\n",
    "    # dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    # db1 = 1 / m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "\n",
    "    gradients = {\n",
    "        'dW' : dWMultiple,\n",
    "        'db' : dbMultiple,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(gradients, parametres, learning_rate):\n",
    "\n",
    "    WMultiple = parametres[0]\n",
    "    bMultiple = parametres[1]\n",
    "\n",
    "    dW = gradients['dW']\n",
    "    db = gradients['db']\n",
    "\n",
    "    for i in range(len(WMultiple)):\n",
    "        WMultiple[i] = WMultiple[i] - learning_rate * dW[i]\n",
    "        bMultiple[i] = bMultiple[i] - learning_rate * db[i]\n",
    "\n",
    "\n",
    "    parametres = {\n",
    "        'W': WMultiple,\n",
    "        'b': bMultiple,\n",
    "    }\n",
    "\n",
    "    return parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parametres):\n",
    "  activations = forward_propagation(X, parametres)\n",
    "  A2 = activations[-1]\n",
    "  return A2 >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  neural_network(X, y, n1=32, nCouche = 3,learning_rate = 0.1, n_iter = 100):\n",
    "\n",
    "\n",
    "     # initialisation parametres\n",
    "    n0 = X.shape[0]\n",
    "    n2 = y.shape[0]\n",
    "    np.random.seed(0)\n",
    "    parametres = initialisation(n0,n1,n2, nCouche)\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    history = []\n",
    "\n",
    "    # gradient descent\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        activations = forward_propagation(X, parametres)\n",
    "        lastActivation = activations[-1]\n",
    "\n",
    "        # Plot courbe d'apprentissage\n",
    "        train_loss.append(log_loss(y.flatten(), lastActivation.flatten()))\n",
    "        y_pred = predict(X, parametres)\n",
    "        train_acc.append(accuracy_score(y.flatten(), y_pred.flatten()))\n",
    "        \n",
    "        history.append([tuple(parametres), train_loss, train_acc, i])\n",
    "\n",
    "        # mise a jour\n",
    "        gradients = back_propagation(X, y, parametres, activations)\n",
    "        parametres = update(gradients, parametres, learning_rate)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='train loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='train acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return parametres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "fictif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=1000, noise=0.1, factor=0.3, random_state=0)\n",
    "X = X.T\n",
    "y = y.reshape((1, y.shape[0]))\n",
    "\n",
    "print('dimensions de X:', X.shape)\n",
    "print('dimensions de y:', y.shape)\n",
    "\n",
    "plt.scatter(X[0, :], X[1, :], c=y, cmap='summer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN - X shape (2, 1000)\n",
      "NN - y shape (1, 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:00<01:10, 141.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 2\n",
      "i 1\n",
      "i 0\n",
      "W shape (32, 32)\n",
      "dZ shape (32, 1000)\n",
      "WMultiple (32, 2)\n",
      "WMultiple (32, 32)\n",
      "WMultiple (1, 32)\n",
      "dWMultiple (32, 2)\n",
      "dWMultiple (32, 32)\n",
      "dWMultiple (1, 32)\n",
      "dbMultiple (32, 1)\n",
      "dbMultiple (32, 1)\n",
      "dbMultiple (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'dot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNN - X shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNN - y shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mneural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 16\u001b[0m, in \u001b[0;36mneural_network\u001b[1;34m(X, y, n1, nCouche, learning_rate, n_iter)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# gradient descent\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_iter)):\n\u001b[1;32m---> 16\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparametres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     lastActivation \u001b[38;5;241m=\u001b[39m activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Plot courbe d'apprentissage\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[76], line 9\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(X, parametres)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(WMultiple)):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m#on calcule les activations de la première couche \u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 9\u001b[0m         ZMultiple \u001b[38;5;241m=\u001b[39m \u001b[43mWMultiple\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m(X) \u001b[38;5;241m+\u001b[39m bMultiple[i]\n\u001b[0;32m     10\u001b[0m         AMultiple\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mZMultiple)))\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m          \u001b[38;5;66;03m#on calcule les activations de la deuxième couche à partir de l'activation de la première couche\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'dot'"
     ]
    }
   ],
   "source": [
    "print(\"NN - X shape\", X.shape)\n",
    "print(\"NN - y shape\", y.shape)\n",
    "neural_network(X, y, n1=32, n_iter=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
